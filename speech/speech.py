import pyaudio
# from faster_whisper import WhisperModel
import numpy as np
from scipy import signal  # â† æ–°å¢ï¼šç”¨æ–¼é‡å–æ¨£
from openwakeword.model import Model
import collections
import wave
import webrtcvad
import time
import requests
import os
import asyncio
import edge_tts
from langdetect import detect
import os
 # server_urlï¼šé€™è£¡è¦å¡«æ¥æ”¶ç«¯çš„ IP (192.168.2.108)

import rospy
from std_msgs.msg import String
import json
import signal as sys_signal
rospy.init_node('speech_publisher', anonymous=True)   # åˆå§‹åŒ– ROS ç¯€é»
    
    # å»ºç«‹ Publisherï¼Œç™¼ä½ˆåˆ° 'visual_object_command' topic
pub = rospy.Publisher('visual_object_command', String, queue_size=10)
task_pub = rospy.Publisher('task_explanation', String, queue_size=10)
task_type_pub = rospy.Publisher('task_type', String, queue_size=10)
time.sleep(1)
rospy.loginfo("èªéŸ³ç™¼ä½ˆå™¨å·²å•Ÿå‹•")


def gripper_response_callback(msg):

    command = msg.data
    asyncio.run(text_to_speech(command))


# send_wav_and_get_response("/home/gairobots/speech/command.wav")
# ============ å–æ¨£ç‡è¨­å®š ============
DEVICE_RATE = 48000      # éº¥å…‹é¢¨ç¡¬é«”æ”¯æ´çš„å–æ¨£ç‡
TARGET_RATE = 16000      # openWakeWord/Whisper éœ€è¦çš„å–æ¨£ç‡
TARGET_CHUNK = 1280      # 80ms @ 16kHz
DEVICE_CHUNK = int(DEVICE_RATE * 0.08)  # 80ms @ 48kHz = 3840 æ¨£æœ¬
DEVICE_INDEX = 5         # å¤–æ¥éº¥å…‹é¢¨ index
DEVICE_CHANNELS = 1      # 2:ç«‹é«”è²
print(f"è£ç½®å–æ¨£ç‡: {DEVICE_RATE} Hz")
print(f"ç›®æ¨™å–æ¨£ç‡: {TARGET_RATE} Hz")
print(f"è£ç½®å€å¡Šå¤§å°: {DEVICE_CHUNK} æ¨£æœ¬")
print(f"ç›®æ¨™å€å¡Šå¤§å°: {TARGET_CHUNK} æ¨£æœ¬\n")

# ============ å…¨å±€åˆå§‹åŒ– ============
oww_model = Model(wakeword_models=["/home/gairobots/camera/speech/hey_anna.onnx"], inference_framework='onnx')
# command_model = WhisperModel("medium", device="cpu", compute_type="int8")
vad = webrtcvad.Vad(1)

print("æ­£åœ¨åˆå§‹åŒ–éŸ³è¨Šè¨­å‚™...")
audio = pyaudio.PyAudio()

# é ç†±æ¨¡å‹ï¼ˆç”¨ 16kHz æ•¸æ“šï¼‰
print("é ç†±æ¨¡å‹...")
dummy_audio = np.zeros(TARGET_CHUNK, dtype=np.int16)
_ = oww_model.predict(dummy_audio)
print("âœ“ æ¨¡å‹å·²å°±ç·’\n")


# å…¨å±€éŸ³è¨Šä¸²æµ
main_stream = None
last_detection_time = 0  # â† æ–°å¢ï¼šå…¨å±€å†·å»æ™‚é–“è¨˜éŒ„
exit_flag = False  # é€€å‡º
def signal_handler(signum, frame):
    """è™•ç† Ctrl+C """
    global exit_flag
    print("\n\næ”¶åˆ°ä¸­æ–­ (Ctrl+C)ï¼Œæ­£åœ¨å®‰å…¨é—œé–‰...")
    exit_flag = True  # â† è®¾ç½®é€€å‡ºæ ‡å¿—

sys_signal.signal(sys_signal.SIGINT, signal_handler)
sys_signal.signal(sys_signal.SIGTERM, signal_handler)

def publisher_object_name(visual_object_name):
    """ç™¼ä½ˆç‰©é«”åç¨±æŒ‡ä»¤"""
    msg = String()
    # msg.data = visual_object_name
    msg.data = json.dumps(visual_object_name)  # è½‰æ›ç‚º JSON å­—ä¸²(ex '["water bottle", "cup"]')
    pub.publish(msg)
    rospy.loginfo(f"ç™¼é€ç‰©é«”åç¨±æŒ‡ä»¤: {visual_object_name}")
    rospy.sleep(0.1)  # ç¢ºä¿è¨Šæ¯ç™¼é€å®Œæˆ
    
def publisher_task_explanation(task):
    msg = String()
    msg.data = task
    task_pub.publish(msg)
    rospy.loginfo(f"ç™¼é€ä»»å‹™è§£é‡‹: {task}")
    rospy.sleep(0.1)  # ç¢ºä¿è¨Šæ¯ç™¼é€å®Œæˆ


def publisher_task_type(task_type):
    msg = String()
    msg.data = task_type
    task_type_pub.publish(msg)
    rospy.loginfo(f"ç™¼é€ä»»å‹™é¡å‹: {task_type}")
    rospy.sleep(0.1)  # ç¢ºä¿è¨Šæ¯ç™¼é€å®Œæˆ

async def text_to_speech(answer):
    """ç•°æ­¥ï¼šæ–‡å­—è½‰èªéŸ³ä¸¦æ’­æ”¾"""
    try:
        lang = detect(answer)
        print(f"Detected language: {lang}")
        
        if lang in ['zh-cn', 'zh-tw', 'zh']:
            # voice = "zh-CN-XiaoxiaoNeural" 
            voice = "zh-TW-HsiaoChenNeural"
        elif lang == 'en':
            voice = "en-US-JennyNeural"
        else:
            voice = "zh-TW-HsiaoChenNeural"  # é è¨­è‹±æ–‡
        
        communicate = edge_tts.Communicate(answer, voice)
        await communicate.save("output.mp3")
        
        print("ğŸ”Š æ’­æ”¾èªéŸ³ä¸­...")
        os.system("ffplay -nodisp -autoexit output.mp3")
        
    except Exception as e:
        print(f"èªéŸ³åˆæˆéŒ¯èª¤: {e}")




def send_wav_and_get_response(file_path):
    server_url = "http://192.168.2.108:9000/upload_wav"
    try:
        with open(file_path, 'rb') as f:
            files = {'file': (file_path, f, 'audio/wav')}
            response = requests.post(server_url, files=files)
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ“ ä¸Šå‚³æˆåŠŸ")
            print(f"è¾¨è­˜çµæœ: {result['user_command']}")
            print(f"LLM å›è¦†: {result['response']}")  # é€™è£¡æ‹¿åˆ°å›è¦†
            llm_response = result.get('response', '')
            if llm_response:
                    asyncio.run(text_to_speech(llm_response))
            return result
        else:
            print(f"âœ— å¤±æ•—: {response.text}")
            return None
    except Exception as e:
        print(f"âœ— éŒ¯èª¤: {e}")
        return None




def init_main_stream():
    """åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–ä¸»éŸ³è¨Šä¸²æµï¼ˆ48kHzï¼‰"""
    global main_stream
    
    # å¦‚æœä¸²æµå·²å­˜åœ¨ä¸”é–‹å•Ÿï¼Œå…ˆé—œé–‰
    if main_stream is not None:
        try:
            if main_stream.is_active():
                main_stream.stop_stream()
            main_stream.close()
        except:
            pass
    
    # é–‹å•Ÿæ–°ä¸²æµï¼ˆ48kHzï¼‰
    main_stream = audio.open(
        format=pyaudio.paInt16,
        channels=DEVICE_CHANNELS,
        rate=DEVICE_RATE,           # â† æ”¹ç”¨ 48kHz
        input=True,
        input_device_index=DEVICE_INDEX,
        frames_per_buffer=DEVICE_CHUNK  # â† æ”¹ç”¨ 3840
    )
    
    # æ¸…ç©ºåˆå§‹ç·©è¡å€
    for _ in range(10):
        try:
            main_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
        except:
            pass
    
    return main_stream


# åˆå§‹åŒ–ä¸»ä¸²æµ
print("æ¸…ç©ºéŸ³è¨Šç·©è¡å€...")
init_main_stream()
print("âœ“ éŸ³è¨Šä¸²æµå·²å°±ç·’\n")

def detect_wake_word():
    """ä½¿ç”¨ Anna æ¨¡å‹æª¢æ¸¬å–šé†’è©ï¼ˆ48kHz â†’ 16kHzï¼‰"""
    global main_stream
    global last_detection_time  # â† æ–°å¢ï¼šè²æ˜ä½¿ç”¨å…¨å±€è®Šæ•¸
    
    print("ç­‰å¾…å–šé†’è©: 'ANNA'ï¼ˆæŒ‰ Ctrl+C é€€å‡ºï¼‰...")
    print("ï¼ˆé¡¯ç¤ºå³æ™‚æª¢æ¸¬åˆ†æ•¸ï¼Œç”¨æ–¼èª¿è©¦ï¼‰\n")
    
    cooldown_period = 2.0
    frame_count = 0
    
    while True:
        if exit_flag:
            print("\næ£€æµ‹åˆ°é€€å‡ºä¿¡å·...")
            return False
        try:
            if not main_stream.is_active():
                print("åµæ¸¬åˆ°ä¸²æµé—œé–‰ï¼Œé‡æ–°åˆå§‹åŒ–...")
                init_main_stream()
            try:
                data = main_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
                audio_raw = np.frombuffer(data, dtype=np.int16)
            except KeyboardInterrupt:
                print("\næ•è·åˆ° Ctrl+Cï¼Œç«‹å³é€€å‡º...")
                raise  
            
            
            # ç«‹é«”è² â†’ å–®è²é“
            if DEVICE_CHANNELS == 2:
                audio_48k = audio_raw.reshape(-1, 2).mean(axis=1).astype(np.int16)
            else:
                audio_48k = audio_raw
            
            # é‡å–æ¨£åˆ° 16kHz
            audio_16k = signal.resample(audio_48k, TARGET_CHUNK).astype(np.int16)
            
            # ç”¨ 16kHz éŸ³è¨Šé æ¸¬
            prediction = oww_model.predict(audio_16k)
            current_time = time.time()
            frame_count += 1
            
            for model_name, score in prediction.items():
                # ===== æ”¹é€²ï¼šé¡¯ç¤ºå†·å»æœŸç‹€æ…‹ =====
                if score > 0.15:
                    # æª¢æŸ¥æ˜¯å¦åœ¨å†·å»æœŸ
                    if (current_time - last_detection_time) < cooldown_period:
                        remaining = cooldown_period - (current_time - last_detection_time)
                        print(f"[å†·å»ä¸­ {remaining:.1f}s] åˆ†æ•¸: {score:.3f}  ", end='\r')
                    else:
                        print(f"[å³æ™‚åˆ†æ•¸] {score:.3f} ", end='\r')
                
                # ===== åœ¨å†·å»æœŸå…§è·³é =====
                if (current_time - last_detection_time) < cooldown_period:
                    continue
                
                if score > 0.45:
                    print(f"\nâœ“ æª¢æ¸¬åˆ° '{model_name}'! (ä¿¡å¿ƒåº¦: {score:.2f})")
                    
                    # æ¸…ç©ºç·©è¡å€
                    for _ in range(10):
                        try:
                            main_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
                        except:
                            break
                    
                    last_detection_time = current_time  # â† æ›´æ–°å…¨å±€å†·å»æ™‚é–“
                    return True
        except KeyboardInterrupt:
            # ç«‹å³é€€å‡ºï¼Œä¸å¤„ç†
            raise
        except Exception as e:
            if "Stream not open" in str(e):
                print("\nä¸²æµç•°å¸¸ï¼Œé‡æ–°åˆå§‹åŒ–...")
                init_main_stream()
            else:
                raise



def write_wave(path, frames):
    """å¯«å…¥ WAV æª”æ¡ˆï¼ˆ16kHzï¼‰"""
    wf = wave.open(path, 'wb')
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(TARGET_RATE)  # â† ç”¨ 16kHz
    wf.writeframes(b''.join(frames))
    wf.close()
    return path


def record_command():
    """éŒ„è£½ä½¿ç”¨è€…å‘½ä»¤ï¼ˆ48kHz â†’ 16kHzï¼‰"""
    print("é–‹å§‹éŒ„éŸ³ï¼Œè«‹èªªå‡ºæŒ‡ä»¤...")
    
    record_stream = None
    wavfile_path = None
    
    try:
        # ===== éŒ„éŸ³ä¸²æµä¹Ÿç”¨ 48kHz =====
        record_stream = audio.open(
            format=pyaudio.paInt16,
            channels=DEVICE_CHANNELS,  # â† æ”¹æˆ 2
            rate=DEVICE_RATE,
            input=True,
            input_device_index=DEVICE_INDEX,
            frames_per_buffer=DEVICE_CHUNK
        )

        
        # æ¸…ç©ºåˆå§‹ç·©è¡å€
        for _ in range(5):
            record_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
        
        audio_buffer = []  # å­˜æ”¾ 16kHz éŸ³è¨Š
        voiced_frames = collections.deque(maxlen=int(1.0 / 0.08))  # æ”¹æˆ 80ms ç‚ºå–®ä½
        triggered = False
        silence_duration = 0
        
        while True:
            # # ===== è®€å– 48kHz éŸ³è¨Š =====
            
            data = record_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
            audio_raw = np.frombuffer(data, dtype=np.int16)

            # ===== ç«‹é«”è² â†’ å–®è²é“ =====
            if DEVICE_CHANNELS == 2:
                audio_48k = audio_raw.reshape(-1, 2).mean(axis=1).astype(np.int16)
            else:
                audio_48k = audio_raw

            # ===== é‡å–æ¨£åˆ° 16kHz =====
            audio_16k = signal.resample(audio_48k, TARGET_CHUNK).astype(np.int16)

            frame_16k = audio_16k.tobytes()
            
            # ===== VAD åˆ¤æ–·ï¼ˆç”¨ 16kHz éŸ³è¨Šï¼‰=====
            # is_speech_frame = vad.is_speech(frame_16k, TARGET_RATE)
            # ===== VAD åˆ¤æ–·ï¼ˆåªç”¨å‰ 30ms = 480 æ¨£æœ¬ï¼‰=====
            vad_chunk = audio_16k[:480].tobytes()  # å–å‰ 480 æ¨£æœ¬
            try:
                is_speech_frame = vad.is_speech(vad_chunk, TARGET_RATE)
            except:
                is_speech_frame = False  # VAD å¤±æ•—æ™‚ç•¶ä½œç„¡èªéŸ³

            
            if not triggered:
                voiced_frames.append((frame_16k, is_speech_frame))
                num_voiced = sum(1 for _, speech in voiced_frames if speech)
                
                if num_voiced > 0.8 * voiced_frames.maxlen:
                    triggered = True
                    # æŠŠä¹‹å‰çš„å¹€éƒ½åŠ é€²å»
                    for f, _ in voiced_frames:
                        audio_buffer.append(f)
                    voiced_frames.clear()
                    print("åµæ¸¬åˆ°èªéŸ³...")
            else:
                audio_buffer.append(frame_16k)
                
                if not is_speech_frame:
                    silence_duration += 0.08  # 80ms
                    if silence_duration > 1.5:
                        print("éŒ„éŸ³å®Œæˆï¼Œæ­£åœ¨ä¿å­˜...")
                        wavfile_path = write_wave("command.wav", audio_buffer)
                        print(f"âœ“ éŸ³è¨Šå·²ä¿å­˜åˆ°: {wavfile_path}")
                        break
                else:
                    silence_duration = 0
    
    except Exception as e:
        print(f"éŒ„éŸ³æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    finally:
        # ç¢ºä¿é—œé–‰éŒ„éŸ³ä¸²æµ
        if record_stream is not None:
            try:
                if record_stream.is_active():
                    record_stream.stop_stream()
                record_stream.close()
            except:
                pass
        
        return wavfile_path



def cleanup():
    """æ¸…ç†èµ„æºï¼ˆå¼ºåŒ–ç‰ˆ - ç¡®ä¿éº¦å…‹é£å®Œå…¨é‡Šæ”¾ï¼‰"""
    global main_stream, audio
    
    print("\næ­£åœ¨å…³é—­éŸ³é¢‘è®¾å¤‡...")
    
    # 1. å…³é—­ä¸»æµ
    if main_stream is not None:
        try:
            if main_stream.is_active():
                main_stream.stop_stream()
            main_stream.close()
            print("âœ“ ä¸»éŸ³é¢‘æµå·²å…³é—­")
        except Exception as e:
            print(f"å…³é—­ä¸»æµæ—¶è­¦å‘Š: {e}")
        finally:
            main_stream = None
    
    # ===== æ·»åŠ å»¶è¿Ÿï¼Œç­‰å¾… ALSA é‡Šæ”¾ =====
    import time
    time.sleep(0.5)
    # =====================================
    
    # 2. ç»ˆæ­¢ PyAudio
    try:
        audio.terminate()
        print("âœ“ PyAudio å·²ç»ˆæ­¢")
    except Exception as e:
        print(f"ç»ˆæ­¢ PyAudio æ—¶è­¦å‘Š: {e}")
    
    # ===== å†æ¬¡å»¶è¿Ÿ =====
    time.sleep(0.5)
    # ====================
    
    # 3. å¼ºåˆ¶é‡Šæ”¾ ALSA éŸ³é¢‘è®¾å¤‡
    try:
        import subprocess
        subprocess.run("fuser -k /dev/snd/* 2>/dev/null", shell=True, timeout=2)
        print("âœ“ å·²å¼ºåˆ¶é‡Šæ”¾éŸ³é¢‘è®¾å¤‡")
    except:
        pass
    
    # ===== æœ€åå»¶è¿Ÿï¼Œç¡®ä¿è®¾å¤‡å®Œå…¨ç©ºé—² =====
    time.sleep(1.0)
    # =======================================
    
    print("âœ“ ç¨‹åºå·²å®‰å…¨é€€å‡º")
    
    import os
    os._exit(0)

def send_wav_file(file_path):
    """
    ç™¼é€ WAV æª”æ¡ˆåˆ° server ä¸¦å–å¾—å›æ‡‰
    
    Args:
        file_path: WAV æª”æ¡ˆè·¯å¾‘
    
    Returns:
        dict: åŒ…å« user_command å’Œ response çš„å­—å…¸ï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    server_url = "http://192.168.2.108:9000/upload_wav"
    
    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"âœ— éŒ¯èª¤: æª”æ¡ˆä¸å­˜åœ¨ - {file_path}")
            return None
        
        # é–‹å•Ÿæª”æ¡ˆä¸¦ä¸Šå‚³
        with open(file_path, 'rb') as f:
            files = {'file': (os.path.basename(file_path), f, 'audio/wav')}
            
            print(f"ğŸ“¤ æ­£åœ¨ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ...")
            response = requests.post(
                server_url, 
                files=files,
                timeout=120  # 2åˆ†é˜è¶…æ™‚ï¼ˆé…åˆ Whisper è™•ç†æ™‚é–“ï¼‰
            )
        
        # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
        if response.status_code == 200:
            result = response.json()
            print(f"âœ“ ä¸Šå‚³æˆåŠŸ")
            print(f"è¾¨è­˜çµæœ: {result['user_command']}")
            print(f"LLM å›è¦†: {result['response']}")
            print(f"visual task: {result.get('visual_task', 'ç„¡')}")
            
            # å¦‚æœæœ‰ LLM å›è¦†ï¼Œæ’­æ”¾èªéŸ³
            llm_response = result.get('response', '')
            visual_task = result.get('visual_task', '')
            if llm_response:
                asyncio.run(text_to_speech(llm_response))
                if visual_task and visual_task != '':
                    visual_object_name = visual_task['target_objects']
                    publisher_object_name(visual_object_name)
                    task_explanation = visual_task['explanation']
                    publisher_task_explanation(task_explanation)
                    task_type = visual_task['task_type']
                    publisher_task_type(task_type)
            
            return result
        else:
            print(f"âœ— ä¸Šå‚³å¤±æ•— (HTTP {response.status_code})")
            print(f"éŒ¯èª¤è¨Šæ¯: {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        print(f"âœ— éŒ¯èª¤: è«‹æ±‚è¶…æ™‚ï¼ˆå¯èƒ½ Whisper è™•ç†æ™‚é–“éé•·ï¼‰")
        return None
    except requests.exceptions.ConnectionError:
        print(f"âœ— éŒ¯èª¤: ç„¡æ³•é€£ç·šåˆ° server ({server_url})")
        print(f"   è«‹ç¢ºèª server æ˜¯å¦åŸ·è¡Œä¸­")
        return None
    except Exception as e:
        print(f"âœ— ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å¾ªç’°"""
    global main_stream
    global last_detection_time
    global exit_flag 
    print("=" * 50)
    print("èªéŸ³åŠ©æ‰‹å·²å•Ÿå‹• (Anna Wake Word)")
    print("å–šé†’è©: 'ANNA'")
    print(f"ç¡¬é«”å–æ¨£ç‡: {DEVICE_RATE} Hz")
    print(f"è™•ç†å–æ¨£ç‡: {TARGET_RATE} Hz")
    print("æç¤º: æŒ‰ Ctrl+C å¯ä»¥éš¨æ™‚é€€å‡ºç¨‹å¼")
    print("=" * 50)
    
    try:
        while True:
            if exit_flag:
                print("ä¸»å¾ªç¯æ”¶åˆ°é€€å‡ºä¿¡å·...")
                break
            if detect_wake_word():
                print("é‡Šæ”¾éº¦å…‹é£...")
                if main_stream is not None:
                    try:
                        if main_stream.is_active():
                            main_stream.stop_stream()
                        main_stream.close()
                        main_stream = None
                    except:
                        pass
                asyncio.run(text_to_speech("æˆ‘åœ¨è½ï¼Œæ€éº¼äº†"))

                command_path = record_command()
                
                if command_path:
                    print("æ­£åœ¨è¾¨è­˜å‘½ä»¤...")
                    send_wav_file(command_path)
                    # segments, _ = command_model.transcribe(
                    #     command_path, 
                    #     language="zh",
                    #     temperature=0.1,
                    #     vad_filter=True
                    # )
                    # result = "".join([seg.text for seg in segments])
                    # print(f"\nã€è¾¨è­˜çµæœã€‘: {result}\n")
                
                # ===== åŠ å¼·é‡å•Ÿæµç¨‹ =====
                print("é‡æ–°åˆå§‹åŒ–ä¸²æµ...")
                try:
                    # 1. é‡å•Ÿä¸²æµ
                    init_main_stream()
                    
                    # 2. é©—è­‰ä¸²æµæ˜¯å¦æ´»èº
                    if not main_stream.is_active():
                        print("âš  ä¸²æµæœªæ´»èºï¼Œé‡è©¦...")
                        init_main_stream()
                    
                    # 3. ç­‰å¾…ç¡¬é«”ç©©å®š
                    time.sleep(1.5)  # å¾ 1.0 å¢åŠ åˆ° 1.5
                    
                    # 4. å¤§é‡æ¸…ç©ºç·©è¡å€ï¼ˆæ¸…é™¤æ‰€æœ‰æ®˜ç•™ï¼‰
                    print("å¤§é‡æ¸…ç©ºç·©è¡å€...")
                    for i in range(60):
                        try:
                            data = main_stream.read(DEVICE_CHUNK, exception_on_overflow=False)
                            # æ¯ 10 å¹€é¡¯ç¤ºé€²åº¦
                            if i % 10 == 0:
                                print(f"  æ¸…ç©ºé€²åº¦: {i}/60", end="\r")
                        except:
                            break
                    
                    print("âœ“ ä¸²æµå·²é‡ç½®                    ")
                    
                    # 5. ç­‰å¾… 1 ç§’å¾Œå†é‡ç½®å†·å»æ™‚é–“ï¼ˆç¢ºä¿æ™‚é–“å·®ï¼‰
                    time.sleep(1.0)
                    last_detection_time = time.time()
                    print("âœ“ å†·å»æœŸå·²é‡ç½®ï¼ˆ2 ç§’ï¼‰")
                    
                except Exception as e:
                    print(f"é‡æ–°åˆå§‹åŒ–å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # å¤±æ•—æ™‚å¼·åˆ¶é‡å•Ÿ
                    init_main_stream()
                    time.sleep(2.0)
                    last_detection_time = time.time()

                print("-" * 50)
                print("æº–å‚™ä¸‹ä¸€æ¬¡å–šé†’\n")
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ° Ctrl+C ä¸­æ–­...")           
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ...")
    except Exception as e:
        print(f"\nç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup()



if __name__ == "__main__":
    try:
        rospy.Subscriber('voice_command', String, gripper_response_callback)
        main()
    except rospy.ROSInterruptException:
        rospy.loginfo("èªéŸ³ç™¼ä½ˆå™¨å·²é—œé–‰")
    main()
