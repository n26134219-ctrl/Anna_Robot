#!/usr/bin/env python3
"""
ç›¸æ©Ÿåµæ¸¬å™¨é¡åˆ¥ - æ”¯æŒå¤šç›¸æ©Ÿ
ä½¿ç”¨å…¨å±€å…±äº«æ¨¡å‹ï¼Œåªè¼‰å…¥ä¸€æ¬¡
"""
import torch
import cv2
import numpy as np
import os
import random
from groundingdino.util.inference import predict, load_image, annotate
from sklearn.decomposition import PCA
from PIL import Image
import pyrealsense2 as rs
from shared_models import shared_models
import torch.nn.functional as F
import threading
import gc
from scipy.spatial import KDTree
import time
def list_cameras():
        """åˆ—å‡ºæ‰€æœ‰é€£æ¥çš„ RealSense ç›¸æ©Ÿ"""
        ctx = rs.context()
        devices = ctx.query_devices()
        
        print(f"æ‰¾åˆ° {len(devices)} å€‹ RealSense ç›¸æ©Ÿ:")
        for i, device in enumerate(devices):
            serial = device.get_info(rs.camera_info.serial_number)
            name = device.get_info(rs.camera_info.name)
            print(f"  [{i}] åºåˆ—è™Ÿ: {serial}, åç¨±: {name}")
        
        return devices
class CameraDetector:
    """å–®å€‹ç›¸æ©Ÿçš„åµæ¸¬å™¨ - ä½¿ç”¨å…¨å±€å…±äº«æ¨¡å‹"""
    # æ·»åŠ é¡ç´šåˆ¥çš„ SAM é–ï¼ˆæ‰€æœ‰å¯¦ä¾‹å…±äº«ï¼‰
    _sam_lock = threading.Lock()
    def __init__(self, realsense_serial="923322070636", camera_id=0, max_objects=2, 
                 candidate_phrases=None):
        """
        åˆå§‹åŒ–ç›¸æ©Ÿåµæ¸¬å™¨
        
        åƒæ•¸:
            realsense_serial: ç›¸æ©Ÿåºåˆ—è™Ÿ
            camera_id: ç›¸æ©Ÿ ID (0:head camera, 1:left camera, 2:right camera)
            max_objects: æœ€å¤šæª¢æ¸¬ç‰©å“æ•¸
            candidate_phrases: æª¢æ¸¬çš„ç‰©å“é¡åˆ¥
        """
        self.camera_serial = realsense_serial
        self.camera_id = camera_id
        self.max_objects = max_objects
        
        # ä½¿ç”¨å…¨å±€å…±äº«æ¨¡å‹
        self.gd_model = shared_models.gd_model
        self.predictor = shared_models.predictor
        self.clip_model = shared_models.clip_model
        self.clip_processor = shared_models.clip_processor
        # self.handle_reference_features = shared_models.handle_reference_features
        self.handle_reference_features = None
        self.device = shared_models.device
        
        # åµæ¸¬åƒæ•¸
        if candidate_phrases is None:
            self.candidate_phrases = [
                "tool", "blue vacuum cleaner", "broom", "dustpan tool", "brush tool"
            ]
        else:
            self.candidate_phrases = candidate_phrases
        
        self.caption = " . ".join(self.candidate_phrases)
        
        # ç›¸æ©Ÿç‹€æ…‹
        self.running = True
        self.latest_color_image = None
        self.latest_depth_frame = None
        self.depth_image = None
        self.detection_count = 0
        self.detected_threshold = 5
        self.objects_info = []
        self.intrinsics = None
        
        # è¼¸å‡ºç›®éŒ„
        self.output_dir = f"/home/gairobots/camera/GroundingDINO/output/camera_{camera_id}"
        os.makedirs(self.output_dir, exist_ok=True)

        # åˆå§‹åŒ–ç›¸æ©Ÿ
        self.init_camera()
        self.camera_started = True
        print(f"\nâœ… ç›¸æ©Ÿ {self.camera_id} (åºåˆ—è™Ÿ: {self.camera_serial}) åˆå§‹åŒ–å®Œæˆ")

    def init_camera(self):
        """åˆå§‹åŒ– RealSense ç›¸æ©Ÿ"""
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_device(self.camera_serial)
        config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
        
        self.profile = self.pipeline.start(config)
        self.align = rs.align(rs.stream.color)
        
        # ç²å–ç›¸æ©Ÿå…§åƒ
        self.intrinsics = self.profile.get_stream(
            rs.stream.color
        ).as_video_stream_profile().get_intrinsics()
        
        print(f"  ç›¸æ©Ÿå…§åƒ: fx={self.intrinsics.fx:.1f}, fy={self.intrinsics.fy:.1f}")
        print(f"  é ç†±ä¸­...")
        # è·³éå‰ 40 å¹€
        for i in range(40):
            self.pipeline.wait_for_frames()
       
    def pause_camera(self):
        """æš«åœç›¸æ©Ÿï¼ˆè‡¨æ™‚åœæ­¢ä½†ä¿ç•™è¨­å®šï¼‰"""
        if not self.camera_started:
            return
        
        print(f"[Camera {self.camera_id}] â¸ï¸  æš«åœç›¸æ©Ÿ...")
        
        if self.pipeline:
            self.pipeline.stop()
        
        self.camera_started = False
        print(f"  âœ… ç›¸æ©Ÿå·²æš«åœ")
    def resume_camera(self):
        """æ¢å¾©ç›¸æ©Ÿï¼ˆå¿«é€Ÿé‡å•Ÿï¼‰"""
        if self.camera_started:
            print(f"[Camera {self.camera_id}] ç›¸æ©Ÿå·²åœ¨é‹è¡Œ")
            return
        
        print(f"[Camera {self.camera_id}] â–¶ï¸  æ¢å¾©ç›¸æ©Ÿ...")
        
        # é‡æ–°å•Ÿå‹•ï¼ˆä¿ç•™ä¹‹å‰çš„é…ç½®ï¼‰
        config = rs.config()
        config.enable_device(self.camera_serial)
        config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
        
        self.profile = self.pipeline.start(config)
        self.align = rs.align(rs.stream.color)
        
        # å¿«é€Ÿé ç†±ï¼ˆåªéœ€ 10 å¹€ï¼‰
        for i in range(10):
            self.pipeline.wait_for_frames()
        
        self.camera_started = True
        print(f"  âœ… ç›¸æ©Ÿå·²æ¢å¾©")
    def test_buffer_without_reading(self):
        """æ¸¬è©¦å®Œå…¨ä¸è®€å–æ™‚çš„ç·©è¡å€ç©ç´¯"""
        if not self.camera_started:
            print("âŒ ç›¸æ©Ÿæœªå•Ÿå‹•")
            return
        
        print(f"\n{'='*60}")
        print(f"[æ¸¬è©¦] ç›¸æ©Ÿ {self.camera_id} - å®Œå…¨ä¸è®€å–æ¸¬è©¦")
        print(f"{'='*60}")
        
        # æ­¥é©Ÿ 1ï¼šæ¸…ç©º
        print("æ­¥é©Ÿ 1: æ¸…ç©ºæ‰€æœ‰ç¾æœ‰å¹€...")
        count_before = 0
        while True:
            frames = self.pipeline.poll_for_frames()
            if frames:
                count_before += 1
            else:
                break
        print(f"  æ¸…ç©ºäº† {count_before} å¹€")
        
        # æ­¥é©Ÿ 2ï¼šå®Œå…¨ä¸è®€å–ï¼Œç­‰å¾… 5 ç§’
        print("æ­¥é©Ÿ 2: å®Œå…¨ä¸è®€å–ï¼Œç­‰å¾… 5 ç§’...")
        print("  ï¼ˆå¦‚æœæœ‰å…¶ä»–ç·šç¨‹åœ¨è®€å–ï¼Œç·©è¡å€ä¸æœƒç©ç´¯ï¼‰")
        
        start_time = time.time()
        time.sleep(5.0)
        elapsed = time.time() - start_time
        
        print(f"  å¯¦éš›ç­‰å¾…æ™‚é–“: {elapsed:.3f}s")
        
        # æ­¥é©Ÿ 3ï¼šç«‹å³è¨ˆæ•¸ç·©è¡å€
        print("æ­¥é©Ÿ 3: è¨ˆæ•¸ç·©è¡å€ä¸­çš„å¹€...")
        
        count_after = 0
        timestamps = []
        
        for i in range(300):  # æœ€å¤šæª¢æŸ¥ 300 å¹€
            frames = self.pipeline.poll_for_frames()
            if frames:
                count_after += 1
                ts = frames.get_timestamp()
                timestamps.append(ts)
                
                # åªæ‰“å°å‰ 5 å¹€å’Œæœ€å¾Œ 5 å¹€
                if count_after <= 5 or i >= 295:
                    print(f"  å¹€ {count_after}: timestamp={ts:.2f}ms")
            else:
                break
        
        # çµæœåˆ†æ
        print(f"\n{'='*60}")
        print("æ¸¬è©¦çµæœ:")
        print(f"  ç­‰å¾…æ™‚é–“: {elapsed:.1f}s")
        print(f"  ç·©è¡å€å¹€æ•¸: {count_after}")
        print(f"  ç†è«–å€¼ (30 FPS Ã— 5s): 150 å¹€")
        print(f"  å¯¦éš›æ¯”ä¾‹: {count_after/150*100:.0f}%")
        
        if count_after < 5:
            print(f"\n  âŒ åš´é‡å•é¡Œï¼šå¹¾ä¹æ²’æœ‰å¹€ç©ç´¯ï¼")
            print(f"     å¯èƒ½åŸå› ï¼š")
            print(f"     1. æœ‰å¾Œå°ç·šç¨‹åœ¨æŒçºŒè®€å–ç›¸æ©Ÿ")
            print(f"     2. æœ‰å…¶ä»–ç¨‹åºåœ¨è¨ªå•ç›¸æ©Ÿ")
            print(f"     3. ç›¸æ©Ÿæ²’æœ‰æ­£å¸¸ä¸²æµ")
        elif count_after < 100:
            print(f"\n  âš ï¸ å¹€ç©ç´¯ä¸è¶³ï¼ˆå¯èƒ½æœ‰ç¨‹åºåœ¨è®€å–ï¼‰")
        else:
            print(f"\n  âœ… ç·©è¡å€ç©ç´¯æ­£å¸¸ï¼ˆæ²’æœ‰å…¶ä»–ç¨‹åºå¹²æ“¾ï¼‰")
        
        # æ™‚é–“æˆ³åˆ†æ
        if len(timestamps) > 1:
            time_span = (timestamps[-1] - timestamps[0]) / 1000.0  # ç§’
            print(f"\n  å¹€æ™‚é–“è·¨åº¦: {time_span:.2f}s")
            
            if time_span < 1.0:
                print(f"  âš ï¸ æ™‚é–“è·¨åº¦å¤ªå°ï¼é€™äº›å¯èƒ½æ˜¯èˆŠå¹€")
        
        print(f"{'='*60}\n")
        
        return count_after
    

    
    def check_for_background_threads(self):
        """æª¢æŸ¥æ˜¯å¦æœ‰å¾Œå°ç·šç¨‹åœ¨è®€å–ç›¸æ©Ÿ"""
        import threading
        
        print(f"\n{'='*60}")
        print(f"[è¨ºæ–·] æª¢æŸ¥ç›¸æ©Ÿ {self.camera_id} çš„å¾Œå°ç·šç¨‹")
        print(f"{'='*60}")
        
        # åˆ—å‡ºæ‰€æœ‰æ´»å‹•ç·šç¨‹
        all_threads = threading.enumerate()
        print(f"ç•¶å‰æ´»å‹•ç·šç¨‹æ•¸: {len(all_threads)}")
        
        for thread in all_threads:
            print(f"  - {thread.name} (daemon={thread.daemon}, alive={thread.is_alive()})")
        
        # æª¢æŸ¥é¡æˆå“¡è®Šé‡
        print(f"\næª¢æŸ¥ç›¸æ©Ÿ {self.camera_id} çš„æˆå“¡è®Šé‡:")
        
        if hasattr(self, 'camera_thread'):
            print(f"  âš ï¸ ç™¼ç¾ camera_thread: {self.camera_thread}")
            if self.camera_thread:
                print(f"     - Alive: {self.camera_thread.is_alive()}")
        else:
            print(f"  âœ… ç„¡ camera_thread")
        
        if hasattr(self, 'thread_running'):
            print(f"  âš ï¸ ç™¼ç¾ thread_running: {self.thread_running}")
        else:
            print(f"  âœ… ç„¡ thread_running")
        
        if hasattr(self, 'running'):
            print(f"  ç™¼ç¾ running: {self.running}")
        
        print(f"{'='*60}\n")

        

    

    def get_current_frame(self):
        # self.check_for_background_threads()
        # self.test_buffer_without_reading()
        """
        ç²å–ç•¶å‰å¹€ï¼ˆé©é… RealSense å°éšŠåˆ—è¨­è¨ˆï¼‰
        
        RealSense ç‰¹æ€§ï¼š
        - éšŠåˆ—åªä¿ç•™ 1-2 å¹€ï¼ˆä¸ç©ç´¯ï¼‰
        - é€™æ˜¯è¨­è¨ˆç‰¹æ€§ï¼Œä¸æ˜¯ bug
        - å¿…é ˆç”¨ wait_for_frames() ä¸»å‹•ç­‰å¾…æ–°å¹€
        """
        if not self.camera_started:
            print(f"[Camera {self.camera_id}] âŒ ç›¸æ©Ÿæœªå•Ÿå‹•")
            return None, None
        
        print(f"[Camera {self.camera_id}] ç²å–æœ€æ–°å¹€...")
        
        # ========== æ­¥é©Ÿ 1ï¼šæ¸…ç©ºéšŠåˆ—ï¼ˆåªæœ‰ 1-2 å¹€ï¼‰==========
        flush_count = 0
        while self.pipeline.poll_for_frames():
            flush_count += 1
        print(f"  Flushed {flush_count} frames")
        
        # ========== æ­¥é©Ÿ 2ï¼šä¸»å‹•ç­‰å¾…æ–°å¹€ï¼ˆé—œéµï¼ï¼‰==========
        # ä¸ç”¨ sleep ç­‰å¾…ç©ç´¯ï¼Œè€Œæ˜¯ç”¨ wait ä¸»å‹•ç²å–æ–°å¹€
        print(f"  Waiting for 15 new frames (~0.6s)...")
        
        for i in range(15):
            try:
                # wait_for_frames() æœƒé˜»å¡ç›´åˆ°ç›¸æ©Ÿç”¢ç”Ÿã€Œä¸‹ä¸€å€‹ã€æ–°å¹€
                frames = self.pipeline.wait_for_frames()
                
                # ç¨å¾®å»¶é²ï¼Œè®“ç›¸æ©Ÿæœ‰æ™‚é–“ç”¢ç”Ÿä¸‹ä¸€å¹€
                # 30 FPS = 33.33ms/å¹€ï¼Œç­‰ 40ms ç¢ºä¿æ˜¯æ–°å¹€
                time.sleep(0.040)
                
            except Exception as e:
                print(f"  âš ï¸ ç­‰å¾…å¹€å¤±æ•—: {e}")
                break
        frames = self.pipeline.wait_for_frames()
        aligned_frames = self.align.process(frames)
        
        depth_frame = aligned_frames.get_depth_frame()
        color_frame = aligned_frames.get_color_frame()
        
        if not depth_frame or not color_frame:
            return None, None
        
        self.latest_depth_frame = depth_frame
        self.depth_image = np.asanyarray(depth_frame.get_data())
        self.latest_color_image = np.asanyarray(color_frame.get_data())
        
        return self.latest_color_image, self.depth_image
    
    def depth_to_point_cloud(self, depth_image, mask):
        """æ·±åº¦è½‰ 3D é»é›²"""
        points_3d = []
        fx, fy = self.intrinsics.fx, self.intrinsics.fy
        cx, cy = self.intrinsics.ppx, self.intrinsics.ppy
        # æ‰¾åˆ°é®ç½©ä¸­çš„æ‰€æœ‰åƒç´ 
        ys, xs = np.where(mask > 0)
        
        for x, y in zip(xs, ys):
            depth = depth_image[y, x] / 1000.0 # è½‰æ›ç‚ºå…¬å°º
            
            if depth == 0 or depth > 3.0:
                continue
             # æ·±åº¦è½‰3Dåº§æ¨™ï¼ˆç›¸æ©Ÿåæ¨™ç³»ï¼Œé‡å­”æˆåƒåŸç†ï¼‰
            X = (x - cx) * depth / fx
            Y = (y - cy) * depth / fy
            Z = depth
            
            points_3d.append([X, Y, Z])
        print(f"æœ€å°z: {(min([p[2] for p in points_3d]) if len(points_3d)>0 else 0)}")
        return np.array(points_3d)
    
    def compute_3d_bounding_box(self, points_3d):
        """ä½¿ç”¨ PCA è¨ˆç®— 3D Bounding Box"""
        if len(points_3d) < 10:
            return None
        # è¨ˆç®—è³ªå¿ƒ
        centroid = np.mean(points_3d, axis=0)
        # PCA æ‰¾ä¸»æ–¹å‘
        points_centered = points_3d - centroid
        pca = PCA(n_components=3)
        pca.fit(points_centered)
        # ä¸»è»¸ï¼ˆç‰¹å¾µå‘é‡ï¼‰
        axes = pca.components_ # 3x3 çŸ©é™£
        # å°‡é»æŠ•å½±åˆ°ä¸»è»¸åæ¨™ç³»
        points_rotated = points_centered @ axes.T
        # è¨ˆç®—åœ¨ä¸»è»¸åæ¨™ç³»ä¸­çš„é‚Šç•Œ
        min_bound = np.min(points_rotated, axis=0)
        max_bound = np.max(points_rotated, axis=0)
        # 3D bounding box çš„å°ºå¯¸ï¼ˆé•·å¯¬é«˜ï¼‰
        size = max_bound - min_bound
        # è¨ˆç®—æ–¹å‘è§’åº¦ï¼ˆyaw, pitch, rollï¼‰
        # ä¸»è¦æ–¹å‘å‘é‡
        main_axis = axes[0]# ç¬¬ä¸€ä¸»æˆåˆ†
        # Yaw è§’åº¦ï¼ˆç¹ Z è»¸ï¼ŒXY å¹³é¢ï¼‰
        yaw = np.arctan2(main_axis[1], main_axis[0]) * 180 / np.pi
        # Pitch è§’åº¦ï¼ˆç¹ Y è»¸ï¼ŒXZ å¹³é¢ï¼‰
        pitch = np.arcsin(-main_axis[2]) * 180 / np.pi
        # è¨ˆç®—8å€‹è§’é»ï¼ˆåœ¨ä¸»è»¸åæ¨™ç³»ï¼‰
        corners_local = np.array([
            [min_bound[0], min_bound[1], min_bound[2]],
            [min_bound[0], min_bound[1], max_bound[2]],
            [min_bound[0], max_bound[1], min_bound[2]],
            [min_bound[0], max_bound[1], max_bound[2]],
            [max_bound[0], min_bound[1], min_bound[2]],
            [max_bound[0], min_bound[1], max_bound[2]],
            [max_bound[0], max_bound[1], min_bound[2]],
            [max_bound[0], max_bound[1], max_bound[2]],
        ])
        
        # è½‰å›åŸå§‹åæ¨™ç³»
        corners_world = corners_local @ axes + centroid
        corners_xy = corners_world[:, :2]  # åªå– X, Y
        center_xy = np.mean(corners_xy, axis=0)  # (x_center, y_center)
        kdtree_xy = KDTree(points_3d[:, :2])
        distances, indices = kdtree_xy.query(center_xy, k=10)  # æ‰¾10å€‹æœ€è¿‘é»
        
        nearby_points = points_3d[indices]
        
        if len(nearby_points) > 0:
            center_z = np.mean(nearby_points[:, 2])
        else:
            center_z = centroid[2]
        
        new_center = np.array([center_xy[0], center_xy[1], center_z])
        print(f"      3D BBox ä¸­å¿ƒ Z èª¿æ•´: {centroid[2]:.4f} -> {new_center[2]:.4f} ")
        return {
            'center': new_center,
            'size': size,
            'orientation': {'yaw': yaw, 'pitch': pitch},
            'rotation_matrix': axes,
            'corners': corners_world,
            'volume': np.prod(size)
        }
    
    def project_3d_to_2d(self, point_3d):
        """3D æŠ•å½±åˆ° 2D åœ–åƒåº§æ¨™"""
        X, Y, Z = point_3d
        if Z == 0:
            return None
        
        u = int(X * self.intrinsics.fx / Z + self.intrinsics.ppx)
        v = int(Y * self.intrinsics.fy / Z + self.intrinsics.ppy)
        
        return (u, v)
    
    def visualize_3d_bbox(self, image, bbox_3d, color=(0, 255, 0)):
        """ç¹ªè£½ 3D Bounding Box"""
        corners = bbox_3d['corners']
        # æŠ•å½±æ‰€æœ‰è§’é»åˆ° 2D
        points_2d = []
        for corner in corners:
            pt_2d = self.project_3d_to_2d(corner)
            if pt_2d:
                points_2d.append(pt_2d)
        
        if len(points_2d) < 8:
            return image
        # ç¹ªè£½ 12 æ¢é‚Šï¼ˆåªç¹ªè£½æ¡†æ¶ï¼‰
        edges = [
            (0, 1), (0, 2), (0, 4), (1, 3), (1, 5),
            (2, 3), (2, 6), (3, 7), (4, 5), (4, 6),
            (5, 7), (6, 7)
        ]
        
        for edge in edges:
            pt1 = points_2d[edge[0]]
            pt2 = points_2d[edge[1]]
            cv2.line(image, pt1, pt2, color, 2)
        
        return image
    
    def compute_iou(self, box1, box2):
        """è¨ˆç®— IoU"""
        x1_min, y1_min, x1_max, y1_max = box1
        x2_min, y2_min, x2_max, y2_max = box2
        
        inter_xmin = max(x1_min, x2_min)
        inter_ymin = max(y1_min, y2_min)
        inter_xmax = min(x1_max, x2_max)
        inter_ymax = min(y1_max, y2_max)
        
        if inter_xmax < inter_xmin or inter_ymax < inter_ymin:
            return 0.0
        
        inter_area = (inter_xmax - inter_xmin) * (inter_ymax - inter_ymin)
        
        box1_area = (x1_max - x1_min) * (y1_max - y1_min)
        box2_area = (x2_max - x2_min) * (y2_max - y2_min)
        union_area = box1_area + box2_area - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0
    
    def non_maximum_suppression(self, detections, iou_threshold=0.5):
        """NMS - ç§»é™¤é‡ç–Šæª¢æ¸¬"""
        if len(detections) == 0:
            return []
        
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        keep = []
        while len(detections) > 0:
            current = detections[0]
            keep.append(current)
            detections = detections[1:]
            
            filtered = []
            for det in detections:
                iou = self.compute_iou(current['bbox_2d'], det['bbox_2d'])
                if iou < iou_threshold:
                    filtered.append(det)
            detections = filtered
        
        return keep
    
    def camera_to_ee_transform(self, point_camera_mm): 
        # horizontal_offset = 29 #mm
        vertical_offset = 57.5 #mm
        
        camera_offset_x = 31 #mm 30
        z_offset = 87.32 #mm 62.32 72.32
        if self.camera_id == 0:
            """é ­ç›¸æ©Ÿåº§æ¨™è½‰è„–å­æœ«ç«¯åº§æ¨™"""
            X_camera, Y_camera, Z_camera = point_camera_mm
            # X_ee = Z_camera
            # Y_ee = -X_camera + head_offset
            # Z_ee = -Y_camera
            # X_ee = Z_camera
            # Y_ee = Y_camera 
            # Z_ee = X_camera
            X_ee = Z_camera
            Y_ee = -X_camera + camera_offset_x
            Z_ee = -Y_camera

            

        elif self.camera_id == 1:
            """å·¦ç›¸æ©Ÿåº§æ¨™è½‰æœ«ç«¯åº§æ¨™"""
            X_camera, Y_camera, Z_camera = point_camera_mm
            X_ee = -X_camera + camera_offset_x
            Y_ee = -Y_camera + vertical_offset
            Z_ee =  Z_camera - z_offset
            # X_ee = Y_camera + horizontal_offset
            # Y_ee = X_camera + vertical_offset
            # Z_ee =  Z_camera - z_offset

        elif self.camera_id == 2:
            """å³ç›¸æ©Ÿåº§æ¨™è½‰æœ«ç«¯åº§æ¨™"""
            X_camera, Y_camera, Z_camera = point_camera_mm
            X_ee = -X_camera + camera_offset_x
            Y_ee = -Y_camera + vertical_offset
            Z_ee = Z_camera - z_offset
            # X_ee = Y_camera + horizontal_offset
            # Y_ee = X_camera + vertical_offset
            # Z_ee =  Z_camera - z_offset
        return (X_ee, Y_ee, Z_ee)
    
    
    def compute_handle_similarity(self, crop_img):
        """è¨ˆç®—æ¡æŸ„ç›¸ä¼¼åº¦ï¼ˆCLIPï¼‰"""
        if self.handle_reference_features is None:
            return 0.0
        
        # è½‰ PIL
        crop_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)
        crop_pil = Image.fromarray(crop_rgb)
        
        # CLIP ç‰¹å¾µæå–
        inputs = self.clip_processor(images=crop_pil, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            crop_features = self.clip_model.get_image_features(
                pixel_values=inputs["pixel_values"]
            )
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(
            self.handle_reference_features, crop_features, dim=-1
        ).item()
        
        return similarity
    
    def find_handle_by_sliding_window(self, object_crop, step_size=20, window_sizes=None):
        """æ»‘å‹•çª—å£æœç´¢æ¡æŸ„"""
        if self.handle_reference_features is None:
            print("    âš ï¸  æ²’æœ‰è¼‰å…¥æ¡æŸ„åƒè€ƒç‰¹å¾µ")
            return None
        h, w = object_crop.shape[:2]
        
        if window_sizes is None:
            window_sizes = [(100, 100), (120, 120), (180, 180)]
            # window_sizes = [(200,200), (250,250), (300,300)]
            # window_sizes = [(h, w), (180,180)]
            # window_sizes = [
            #     (int(h * 0.3), 180),  # ä¸­çª—å£
            
                
                
            #     (180, 180),
            # ]
    
        best_similarity = 0.0
        best_bbox = None
        best_size = None
        
        print(f"    æ»‘å‹•çª—å£æœç´¢æ¡æŸ„ ({h}x{w})...")
        
        for win_h, win_w in window_sizes:
            if win_h >= h or win_w >= w:
                continue
            
            # æ»‘å‹•çª—å£
            for y in range(0, h - win_h, step_size):
                for x in range(0, w - win_w, step_size):
                    # æå–çª—å£
                    window_crop = object_crop[y:y+win_h, x:x+win_w].copy()
                    
                    # è¨ˆç®—ç›¸ä¼¼åº¦
                    similarity = self.compute_handle_similarity(window_crop)
                    
                    # æ›´æ–°æœ€ä½³
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_bbox = (x, y, x + win_w, y + win_h)
                        best_size = (win_w, win_h)
            print(f"      çª—å£ ({win_h}x{win_w}) : ç›¸ä¼¼åº¦ {similarity:.3f}")
        
        print(f"    çª—å£å¤§å°: {best_size}, æœ€ä½³ç›¸ä¼¼åº¦: {best_similarity:.3f}")
        
        if best_similarity > 0.35 and best_bbox is not None:
            print(f"    âœ“ æ‰¾åˆ°æ¡æŸ„ (ç›¸ä¼¼åº¦: {best_similarity:.3f}, å°ºå¯¸: {best_size})")
            return {
                'bbox': best_bbox,
                'similarity': best_similarity,
                'size': best_size
            }
        else:
            print(f"    âŒ æ¡æŸ„æœç´¢å¤±æ•— (æœ€ä½³ç›¸ä¼¼åº¦: {best_similarity:.3f})")
            return None


    def _get_initial_detections(self, image_source, image):
        self.caption = " . ".join(self.candidate_phrases)
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ GroundingDINO é€²è¡Œåˆå§‹åµæ¸¬"""
        boxes, logits, phrases = predict(
            self.gd_model, image, self.caption, 0.20, 0.10, self.device
        )
        
        print(f"   GroundingDINO åµæ¸¬åˆ° {len(boxes)} å€‹å€™é¸æ¡†")
        # ===== æ”¶é›†æ‰€æœ‰å€™é¸æª¢æ¸¬ï¼Œä¸¦æŒ‰ä¿¡å¿ƒåº¦æ’åº =====
        all_detections = []
        H, W = image_source.shape[:2]
        
        for i, (box, logit, phrase) in enumerate(zip(boxes, logits, phrases)):
            if logit < 0.25: # éæ¿¾ä½ä¿¡å¿ƒåº¦
                continue
            # è½‰åƒç´ åº§æ¨™
            cx, cy, w, h = box
            x1 = int((cx - w/2) * W)
            y1 = int((cy - h/2) * H)
            x2 = int((cx + w/2) * W)
            y2 = int((cy + h/2) * H)
            
            all_detections.append({
                'box': box,
                'bbox_2d': (x1, y1, x2, y2),
                'logit': float(logit),
                'phrase': phrase,
                'confidence': float(logit)
            })
        
        return all_detections
    
    def _filter_detections(self, all_detections):
        """ç¬¬äºŒæ­¥ï¼šNMS å’Œç¯©é¸é ‚éƒ¨æª¢æ¸¬"""
        if len(all_detections) == 0:
            print("æœªåµæ¸¬åˆ°ä»»ä½•ç‰©å“")
            return []
        
        all_detections = self.non_maximum_suppression(all_detections, iou_threshold=0.5)
         # é¸æ“‡å‰ k å€‹ä¿¡å¿ƒåº¦æœ€é«˜çš„æª¢æ¸¬
        selected_detections = all_detections[:self.max_objects]
        
        print(f"   ç¶“é NMS å’Œç¯©é¸å¾Œï¼Œå‰©é¤˜ {len(selected_detections)} å€‹æª¢æ¸¬")
        
        return selected_detections
    
    def _detect_object_region(self, image_source, x1, y1, x2, y2, det_idx):
        """ç¬¬ä¸‰æ­¥ï¼šæª¢æ¸¬æ•´é«”ç‰©é«”å€åŸŸ"""
        print(f"      ç¬¬ä¸€éšæ®µï¼šæ•´é«”ç‰©é«” 3D bbox...")
        
        crop_image = image_source[y1:y2, x1:x2].copy()
        masks_obj, _, _ = self.predictor.predict(
            box=np.array([x1, y1, x2, y2]),
            multimask_output=False
        )
        object_mask = masks_obj[0].astype(np.uint8) * 255
        
        mask_path = os.path.join(self.output_dir, f"mask_object_{det_idx}.jpg")
        cv2.imwrite(mask_path, object_mask)
        
        self.points_3d_object = self.depth_to_point_cloud(
            self.depth_image, object_mask
        )
        
        if len(self.points_3d_object) < 10:
            print(f"      âŒ ç‰©é«”é»é›²ä¸è¶³ï¼Œè·³é")
            return None, None, None
        
        bbox_3d_object = self.compute_3d_bounding_box(self.points_3d_object)
        
        if bbox_3d_object is None:
            print(f"      âŒ ç„¡æ³•è¨ˆç®—æ•´é«”ç‰©é«” 3D bboxï¼Œè·³é")
            return None, None, None
        
        print(f"      âœ“ æ•´é«”ç‰©é«” 3D bbox è¨ˆç®—æˆåŠŸ ({len(self.points_3d_object)} å€‹é»)")
        
        return bbox_3d_object, object_mask, self.points_3d_object, crop_image
    
    def _detect_handle_region(self, image_source, crop_image, x1, y1, x2, y2, 
                             bbox_3d_object, object_mask, points_3d_object, det_idx):
        """ç¬¬å››æ­¥ï¼šæœç´¢æ¡æŸ„å€åŸŸ"""
        print(f"      ç¬¬äºŒéšæ®µï¼šCLIP æ»‘å‹•çª—å£æœç´¢æ¡æŸ„...")
        
        handle_result = self.find_handle_by_sliding_window(crop_image, step_size=10)
        use_handle = False
        
        if handle_result is not None:
            handle_x1_crop, handle_y1_crop, handle_x2_crop, handle_y2_crop = handle_result['bbox']
            similarity_score = handle_result['similarity']
            
            handle_x1 = x1 + handle_x1_crop
            handle_y1 = y1 + handle_y1_crop
            handle_x2 = x1 + handle_x2_crop
            handle_y2 = y1 + handle_y2_crop
            
            print(f"      âœ“ æ‰¾åˆ°æ¡æŸ„å€åŸŸ: ({handle_x1}, {handle_y1}) -> ({handle_x2}, {handle_y2})")
            print(f"      æ¡æŸ„ CLIP ç›¸ä¼¼åº¦: {similarity_score:.3f}")
            with torch.no_grad():
                masks_handle, _, _ = self.predictor.predict(
                    box=np.array([handle_x1, handle_y1, handle_x2, handle_y2]),
                    multimask_output=False
                )
            handle_mask = masks_handle[0].astype(np.uint8) * 255
            del masks_handle
            gc.collect()
            torch.cuda.empty_cache()
            handle_mask_path = os.path.join(
                self.output_dir, f"mask_handle_final_{det_idx}.jpg"
            )
            cv2.imwrite(handle_mask_path, handle_mask)
            
            handle_points_3d = self.depth_to_point_cloud(
                self.depth_image, handle_mask
            )
            
            if len(handle_points_3d) >= 10:
                handle_bbox_3d = self.compute_3d_bounding_box(handle_points_3d)
                
                if handle_bbox_3d is not None:
                    print(f"      âœ“ æ¡æŸ„ 3D bbox è¨ˆç®—æˆåŠŸ ({len(handle_points_3d)} å€‹é»)")
                    use_handle = True
                    
                    return {
                        'bbox_3d': handle_bbox_3d,
                        'mask': handle_mask,
                        'bbox_2d': (handle_x1, handle_y1, handle_x2, handle_y2),
                        'points_3d': handle_points_3d,
                        'region_type': 'handle'
                    }
                else:
                    print(f"      âš ï¸  ç„¡æ³•è¨ˆç®—æ¡æŸ„ 3D bboxï¼Œæ”¹ç”¨æ•´å€‹ç‰©é«”")
            else:
                print(f"      âš ï¸  æ¡æŸ„é»é›²ä¸è¶³ï¼Œæ”¹ç”¨æ•´å€‹ç‰©é«”")
        else:
            print(f"      âš ï¸  CLIP æœç´¢æœªæ‰¾åˆ°æ¡æŸ„ï¼Œæ”¹ç”¨æ•´å€‹ç‰©é«”")
        
        # å›é€€åˆ°æ•´é«”ç‰©é«”
        print(f"      ä½¿ç”¨æ•´å€‹ç‰©é«” 3D bounding box")
        return {
            'bbox_3d': bbox_3d_object,
            'mask': object_mask,
            'bbox_2d': (x1, y1, x2, y2),
            'points_3d': points_3d_object,
            'region_type': 'object'
        }
    
    def _extract_3d_info(self, bbox_3d):
        """ç¬¬äº”æ­¥ï¼šæå– 3D è³‡è¨Š"""
        corners = bbox_3d['corners']
        print(f"      3D Bounding Box è§’é»:\n{corners}")
        center_3d = np.mean(corners, axis=0)
        print(f"      3D Bounding Box ä¸­å¿ƒé»: {center_3d}")
    
        center_3d = np.array([center_3d[0] * 1000, center_3d[1] * 1000, center_3d[2] * 1000])
        size_3d = np.array([bbox_3d['size'][0]*1000, bbox_3d['size'][1]*1000, bbox_3d['size'][2]*1000])
   
        yaw = bbox_3d['orientation']['yaw']

        pitch = bbox_3d['orientation']['pitch']
        center_base = self.camera_to_ee_transform(center_3d)
        
        return {
            'center_3d': center_3d,
            'center_base': center_base,
            'size_3d': size_3d,
            'yaw': yaw,
            'pitch': pitch,
            'corners': corners
        }
    
    def _calculate_endpoints(self, bbox_3d, center_3d):
        """ç¬¬å…­æ­¥ï¼šè¨ˆç®—é•·é‚Šå…©ç«¯é»"""
        corners = bbox_3d['corners']
        size_3d = bbox_3d['size']
        axes = bbox_3d['rotation_matrix']
        
        sorted_indices = np.argsort(size_3d)[::-1]
        long_idx = sorted_indices[0]
        long_dir = axes[long_idx]
        
        projections = []
        for i, corner in enumerate(corners):
            vec_to_corner = corner - center_3d
            projection = np.dot(vec_to_corner, long_dir)
            projections.append((projection, i, corner))
        
        projections.sort(key=lambda x: x[0])
        
        left_face_corners = [p[2] for p in projections[:4]]
        right_face_corners = [p[2] for p in projections[4:]]
        
        left_endpoint_3d = np.mean(left_face_corners, axis=0)
        right_endpoint_3d = np.mean(right_face_corners, axis=0)

        left_endpoint_3d = np.array([left_endpoint_3d[0]*1000, left_endpoint_3d[1]*1000, left_endpoint_3d[2]*1000])
        right_endpoint_3d = np.array([right_endpoint_3d[0]*1000, right_endpoint_3d[1]*1000, right_endpoint_3d[2]*1000])


        left_endpoint_base = self.camera_to_ee_transform(left_endpoint_3d)
        right_endpoint_base = self.camera_to_ee_transform(right_endpoint_3d)
        
        max_distance = np.linalg.norm(right_endpoint_3d - left_endpoint_3d)
        edge_direction = right_endpoint_3d - left_endpoint_3d

        edge_yaw = np.arctan2(edge_direction[1], edge_direction[0]) * 180 / np.pi
        
        left_endpoint_2d = self.project_3d_to_2d(left_endpoint_3d)
        right_endpoint_2d = self.project_3d_to_2d(right_endpoint_3d)
        
        return {
            'left_3d': left_endpoint_3d,
            'right_3d': right_endpoint_3d,
            'left_base': left_endpoint_base,
            'right_base': right_endpoint_base,
            'left_2d': left_endpoint_2d,
            'right_2d': right_endpoint_2d,
            'distance': max_distance,
            'edge_yaw': edge_yaw
        }
    
    def _verify_with_clip(self, image_source, final_bbox_2d, final_mask):
        """ç¬¬ä¸ƒæ­¥ï¼šCLIP é©—è­‰"""
        crop = image_source[final_bbox_2d[1]:final_bbox_2d[3], 
                           final_bbox_2d[0]:final_bbox_2d[2]].copy()
        crop_mask_2d = final_mask[final_bbox_2d[1]:final_bbox_2d[3], 
                                  final_bbox_2d[0]:final_bbox_2d[2]]
        crop = cv2.bitwise_and(crop, crop, mask=crop_mask_2d)
        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
        crop_pil = Image.fromarray(crop_rgb)
        
        inputs = self.clip_processor(
            images=crop_pil, text=self.candidate_phrases, 
            return_tensors="pt", padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            img_feat = self.clip_model.get_image_features(
                pixel_values=inputs["pixel_values"]
            )
            txt_feat = self.clip_model.get_text_features(
                input_ids=inputs["input_ids"], 
                attention_mask=inputs["attention_mask"]
            )
            sims = (img_feat @ txt_feat.T).squeeze(0)
            clip_label = self.candidate_phrases[sims.argmax().item()]
        
        return clip_label
    
    def _visualize_detection(self, vis, info, color, W, H, det_idx):
        """ç¬¬å…«æ­¥ï¼šç¹ªè£½æª¢æ¸¬çµæœ"""
        bbox_3d = info['bbox_3d']
        center_base = info['3d_info']['center_base']
        center_3d = info['3d_info']['center_3d']
        size_3d = info['3d_info']['size_3d']
        yaw = info['3d_info']['yaw']
        pitch = info['3d_info']['pitch']
        endpoints = info['endpoints']
        region_type = info['region_type']
        final_label = info['final_label']
        logit = info['logit']
        points_3d = info['points_3d']
        
        # ç¹ªè£½ 3D bbox
        vis = self.visualize_3d_bbox(vis, bbox_3d, color)
        
        # ç¹ªè£½ä¸­å¿ƒé»
        center_2d = self.project_3d_to_2d(center_3d)
        if center_2d is not None:
            cx, cy = center_2d
            cv2.circle(vis, (int(cx), int(cy)), 10, color, -1)
            cv2.circle(vis, (int(cx), int(cy)), 10, (255, 255, 255), 2)
            cv2.putText(vis, "C", (int(cx) - 20, int(cy) - 20), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
        # ç¹ªè£½ç«¯é»
        left_endpoint_2d = endpoints['left_2d']
        right_endpoint_2d = endpoints['right_2d']
        
        if left_endpoint_2d and right_endpoint_2d:
            Lx, Ly = left_endpoint_2d
            Rx, Ry = right_endpoint_2d
            
            cv2.circle(vis, (Lx, Ly), 8, (0, 255, 0), -1)
            cv2.circle(vis, (Rx, Ry), 8, (0, 0, 255), -1)
            cv2.line(vis, (Lx, Ly), (Rx, Ry), (255, 255, 255), 3)
            cv2.putText(vis, "L", (Lx - 20, Ly - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(vis, "R", (Rx + 10, Ry - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ç¹ªè£½æ–‡å­—è³‡è¨Š
        text = (f"#{det_idx+1} {final_label}: {logit:.2f}\n"
            f"Region: {region_type}\n"
            f"Center: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f})mm\n"
            f"Size: ({size_3d[0]:.3f}, {size_3d[1]:.3f}, {size_3d[2]:.3f})mm\n"
            f"Yaw: {yaw:.1f} deg / Edge: {endpoints['edge_yaw']:.1f} deg\n"
            f"Pitch: {pitch:.1f} deg")
        
        font, fs, th = cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1
        lines = text.split('\n')
        line_height = cv2.getTextSize("Test", font, fs, th)[0][1] + 5
        
        tx = info['bbox_2d'][0]
        ty = info['bbox_2d'][1] - 10
        
        max_width = max([cv2.getTextSize(line, font, fs, th)[0][0] for line in lines])
        total_height = len(lines) * line_height
        
        tx = max(0, min(tx, W - max_width - 10))
        ty = max(total_height, min(ty, H - 10))
        
        cv2.rectangle(vis, (tx, ty - total_height), (tx + max_width, ty + 5), 
                    color, cv2.FILLED)
        
        for i, line in enumerate(lines):
            y_pos = ty - total_height + (i + 1) * line_height
            cv2.putText(vis, line, (tx, y_pos), font, fs, (0, 0, 0), th, cv2.LINE_AA)
        
        return vis
    
    def _save_object_info(self, info):
        """ç¬¬ä¹æ­¥ï¼šå„²å­˜ç‰©å“è³‡è¨Š"""
      
        yaw = info['3d_info']['yaw']
        if yaw <0:
            yaw = yaw+180
        if info['3d_info']['size_3d'][2] > info['3d_info']['size_3d'][0] and info['3d_info']['size_3d'][2] > info['3d_info']['size_3d'][1]:
            pick_mode="side"
        else:
            pick_mode="down"
        obj_info = {
            "name": info['final_label'],
            "region_type": info['region_type'],
            "center_pos": tuple(info['3d_info']['center_base']),
            "3d_size": tuple(info['3d_info']['size_3d']),
            "angle":float(yaw),
            "left_endpoint": tuple(info['endpoints']['left_base']),
            "right_endpoint": tuple(info['endpoints']['right_base']),
            "camera_id": self.camera_id,
            "confidence": float(info['logit']),
            "pick_mode":pick_mode,
            "center_vector": tuple(info.get('center_vector', (None, None, None))),
            "endpoints_total_obj": tuple(info.get('endpoints_total_obj', (None, None, None))),
            "longest_length": float(info.get('longest_length', None)),
            "shortest_length": float(info.get('shortest_length', None)),
        }
        
        self.objects_info.append(obj_info)
    
    # ========================================
    # ä¸»è¦åµæ¸¬å‡½æ•¸
    # ========================================
    

    def _calculate_unit_vector(self, pointA, pointB):
        """è¨ˆç®—å…©é»ä¹‹é–“çš„å‘é‡"""
        vector = np.array(pointB) - np.array(pointA)
        norm = np.linalg.norm(vector)
        if norm == 0:
            return vector
        return vector / norm

    def detect_objects(self):
        
        self.handle_reference_features = shared_models.get_handle_features(self.candidate_phrases[0])

        """å°ç•¶å‰ç•«é¢é€²è¡Œç‰©å“åµæ¸¬ä¸¦è¨ˆç®—3Dè³‡è¨Šï¼ˆCLIP æ¡æŸ„æ¯”å°ç‰ˆæœ¬ï¼‰"""
        if self.latest_color_image is None:
            print(f"âŒ ç›¸æ©Ÿ {self.camera_id}: æ²’æœ‰å¯ç”¨çš„å½±åƒé€²è¡Œåµæ¸¬")
            return False
            
        print(f"\nğŸ” ç›¸æ©Ÿ {self.camera_id} é–‹å§‹ç‰©å“åµæ¸¬ï¼ˆæœ€å¤š {self.max_objects} å€‹ç‰©å“ï¼‰...")
        
        # è¼‰å…¥å’Œæº–å‚™å½±åƒ
        temp_image_path = f"/home/gairobots/camera/GroundingDINO/data/tmp/temp_detect_camera_{self.camera_id}.png"
        cv2.imwrite(temp_image_path, self.latest_color_image)
        image_source, image = load_image(temp_image_path)
        
        rgb_image = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)
        gc.collect()
        torch.cuda.empty_cache()
        with self._sam_lock:
            self.predictor.set_image(rgb_image)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åµæ¸¬
        with torch.no_grad():
            all_detections = self._get_initial_detections(image_source, image)
        if len(all_detections) == 0:
            print(f"   âŒ ç›¸æ©Ÿ {self.camera_id}: æœªåµæ¸¬åˆ°ä»»ä½•ç‰©å“")
            return False
        
        # ç¬¬äºŒæ­¥ï¼šç¯©é¸
        selected_detections = self._filter_detections(all_detections)
        
        # è¦–è¦ºåŒ–æº–å‚™
        vis = image_source.copy()
        random.seed(42)
        any_detected = False
        H, W = image_source.shape[:2]
        
        colors_list = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0),
            (0, 128, 255), (128, 255, 0)
        ]
        
        # è™•ç†æ¯å€‹æª¢æ¸¬
        for det_idx, detection in enumerate(selected_detections):
            x1, y1, x2, y2 = detection['bbox_2d']
            logit = detection['logit']
            phrase = detection['phrase']
            color = colors_list[det_idx % len(colors_list)]
            
            print(f"\n   è™•ç†ç¬¬ {det_idx + 1} å€‹ç‰©å“: {phrase}ï¼Œä¿¡å¿ƒåº¦: {logit:.2f}")
            
            # ç¬¬ä¸‰æ­¥ï¼šæª¢æ¸¬æ•´é«”ç‰©é«”
            result = self._detect_object_region(image_source, x1, y1, x2, y2, det_idx)
            if result[0] is None:
                gc.collect()
                torch.cuda.empty_cache()
                continue
            bbox_3d_object, object_mask, points_3d_object, crop_image = result
            
            # ç¬¬å››æ­¥ï¼šæª¢æ¸¬æ¡æŸ„æˆ–å›é€€åˆ°æ•´é«”ç‰©é«”
            region_info = self._detect_handle_region(
                image_source, crop_image, x1, y1, x2, y2,
                bbox_3d_object, object_mask, points_3d_object, det_idx
            )
            
            bbox_3d = region_info['bbox_3d'] # æ¡æŸ„3d bounding box
            final_mask = region_info['mask']
            final_bbox_2d = region_info['bbox_2d']
            points_3d = region_info['points_3d']
            region_type = region_info['region_type']
            
            # ç¬¬äº”æ­¥ï¼šæå– 3D è³‡è¨Š
            info_3d = self._extract_3d_info(bbox_3d)
            gc.collect()
            torch.cuda.empty_cache()

            print(f"      æå– 3D è³‡è¨Šï¼ˆ{region_type}ï¼‰...")
            print(f"      3Dä¸­å¿ƒ: ({info_3d['center_base'][0]:.3f}, {info_3d['center_base'][1]:.3f}, {info_3d['center_base'][2]:.3f}) mm")
            print(f"      3Då°ºå¯¸: ({info_3d['size_3d'][0]:.3f}, {info_3d['size_3d'][1]:.3f}, {info_3d['size_3d'][2]:.3f}) mm")
            
            total_center = np.mean(bbox_3d_object['corners'], axis=0) * 1000.0 #mm
            handle_center = info_3d['center_3d'] 
            center_vector = self._calculate_unit_vector(handle_center, total_center)

            # ç¬¬å…­æ­¥ï¼šè¨ˆç®—ç«¯é»
            endpoints = self._calculate_endpoints(bbox_3d, info_3d['center_3d'])
            endpoints_total_obj = self._calculate_endpoints(bbox_3d_object, total_center)

            print(f"     æ•´é«”ç‰©é«”å·¦ç«¯é»ï¼ˆmmï¼‰: ï¼ˆ{endpoints_total_obj['left_base'][0]:.3f}, {endpoints_total_obj['left_base'][1]:.3f}, {endpoints_total_obj['left_base'][2]:.3f}ï¼‰")
            print(f"     æ•´é«”ç‰©é«”å³ç«¯é»ï¼ˆmmï¼‰: ï¼ˆ{endpoints_total_obj['right_base'][0]:.3f}, {endpoints_total_obj['right_base'][1]:.3f}, {endpoints_total_obj['right_base'][2]:.3f}ï¼‰")            
            print(f"      å·¦ç«¯é»åŸºåº§: ({endpoints['left_base'][0]:.3f}, {endpoints['left_base'][1]:.3f}, {endpoints['left_base'][2]:.3f}) mm")
            print(f"      å³ç«¯é»åŸºåº§: ({endpoints['right_base'][0]:.3f}, {endpoints['right_base'][1]:.3f}, {endpoints['right_base'][2]:.3f}) mm")
            
            # å°‡ tuple è½‰æ›ç‚º numpy array å†è¨ˆç®—
            left_array_base = np.array([endpoints_total_obj['left_base'][0], endpoints_total_obj['left_base'][1]])
            right_array_base = np.array([endpoints_total_obj['right_base'][0], endpoints_total_obj['right_base'][1]])
            center_array_base = np.array([info_3d['center_base'][0], info_3d['center_base'][1]])

            length1 = np.linalg.norm(left_array_base - center_array_base)
            length2 = np.linalg.norm(right_array_base - center_array_base)
            longest_length = max(length1, length2)
            shortest_length = min(length1, length2)
            print(f"      ç‰©é«”ä¸­å¿ƒåˆ°é•·é‚Šç«¯é»æœ€é•·è·é›¢: {longest_length:.3f} mm")
            print(f"      ç‰©é«”ä¸­å¿ƒåˆ°é•·é‚Šç«¯é»æœ€çŸ­è·é›¢: {shortest_length:.3f} mm")
            # ç¬¬ä¸ƒæ­¥ï¼šCLIP é©—è­‰
            print(f"      CLIP é©—è­‰...")
            with torch.no_grad():
                final_label = self._verify_with_clip(image_source, final_bbox_2d, final_mask)
            
            any_detected = True
            
            # æ”¶é›†æ‰€æœ‰è³‡è¨Š
            detection_info = {
                'bbox_3d': bbox_3d,
                'bbox_2d': final_bbox_2d,
                '3d_info': info_3d,
                'endpoints': endpoints,
                'region_type': region_type,
                'final_label': final_label,
                'logit': logit,
                'points_3d': points_3d,
                'center_vector': center_vector,
                'endpoints_total_obj': endpoints_total_obj,
                'longest_length': longest_length,
                'shortest_length': shortest_length,

            }
            
            # ç¬¬å…«æ­¥ï¼šè¦–è¦ºåŒ–
            vis = self._visualize_detection(vis, detection_info, color, W, H, det_idx)
            
            # ç¬¬ä¹æ­¥ï¼šå„²å­˜è³‡è¨Š
            self._save_object_info(detection_info)
        
        # å„²å­˜çµæœ
        vis_bgr = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)
        # cv2.imwrite(result_path, vis_bgr)
        result_path = os.path.join(self.output_dir, "detection_3d.jpg")
        cv2.imwrite(result_path, vis_bgr)

        if not any_detected:
            print(f"   âŒ ç›¸æ©Ÿ {self.camera_id}: æœªåµæ¸¬åˆ°ä»»ä½•ç‰©å“")
            self.clear_detection_data()
            
            return False
        else:
            print(f"\n   âœ… ç›¸æ©Ÿ {self.camera_id}: 3Dåµæ¸¬çµæœå·²å„²å­˜: {result_path}")
            print(f"   æˆåŠŸæª¢æ¸¬åˆ° {len(self.objects_info)} å€‹ç‰©å“")
            self.clear_detection_data()
        
            return True

    
    def detect_objects_simple(self):
        
        """å°ç•¶å‰ç•«é¢é€²è¡Œç‰©å“åµæ¸¬ä¸¦è¨ˆç®—3Dè³‡è¨Šï¼ˆç°¡åŒ–ç‰ˆï¼šåƒ…æ•´é«”ç‰©é«”æª¢æ¸¬ï¼‰"""
        if self.latest_color_image is None:
            print(f"âŒ ç›¸æ©Ÿ {self.camera_id}: æ²’æœ‰å¯ç”¨çš„å½±åƒé€²è¡Œåµæ¸¬")
            return False
            
        print(f"\nğŸ” ç›¸æ©Ÿ {self.camera_id} é–‹å§‹ç‰©å“åµæ¸¬ï¼ˆç°¡åŒ–ç‰ˆ - æœ€å¤š {self.max_objects} å€‹ç‰©å“ï¼‰...")
        print(f"phase list: {self.candidate_phrases}")
        
        # è¼‰å…¥å’Œæº–å‚™å½±åƒ
        temp_image_path = f"/home/gairobots/camera/GroundingDINO/data/tmp/temp_detect_camera_{self.camera_id}.png"
        cv2.imwrite(temp_image_path, self.latest_color_image)
        image_source, image = load_image(temp_image_path)
        
        rgb_image = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)

        gc.collect()
        torch.cuda.empty_cache()
        with self._sam_lock:
            self.predictor.set_image(rgb_image)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åµæ¸¬
        with torch.no_grad():
            all_detections = self._get_initial_detections(image_source, image)
        if len(all_detections) == 0:
            print(f"   âŒ ç›¸æ©Ÿ {self.camera_id}: æœªåµæ¸¬åˆ°ä»»ä½•ç‰©å“")
            return False
        
        # ç¬¬äºŒæ­¥ï¼šç¯©é¸
        selected_detections = self._filter_detections(all_detections)
        
        # è¦–è¦ºåŒ–æº–å‚™
        vis = image_source.copy()
        random.seed(42)
        any_detected = False
        H, W = image_source.shape[:2]
        
        colors_list = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0),
            (0, 128, 255), (128, 255, 0)
        ]
        
        # è™•ç†æ¯å€‹æª¢æ¸¬
        for det_idx, detection in enumerate(selected_detections):
            x1, y1, x2, y2 = detection['bbox_2d']
            logit = detection['logit']
            phrase = detection['phrase']
            color = colors_list[det_idx % len(colors_list)]
            
            print(f"\n   è™•ç†ç¬¬ {det_idx + 1} å€‹ç‰©å“: {phrase}ï¼Œä¿¡å¿ƒåº¦: {logit:.2f}")
            
            # ç°¡åŒ–ç‰ˆï¼šåƒ…æª¢æ¸¬æ•´é«”ç‰©é«”ï¼ˆè·³éæ¡æŸ„æª¢æ¸¬ï¼‰
            print(f"      æª¢æ¸¬æ•´é«”ç‰©é«” 3D bbox...")
            
            # ä½¿ç”¨ SAM ç”Ÿæˆ mask
            with torch.no_grad():
                masks_obj, _, _ = self.predictor.predict(
                    box=np.array([x1, y1, x2, y2]),
                    multimask_output=False
                )
            object_mask = masks_obj[0].astype(np.uint8) * 255

            del masks_obj
            gc.collect()
            torch.cuda.empty_cache()

            mask_path = os.path.join(self.output_dir, f"mask_simple_{det_idx}.jpg")
            cv2.imwrite(mask_path, object_mask)
            
            # è¨ˆç®— 3D bbox
            points_3d = self.depth_to_point_cloud(
                self.depth_image, object_mask
            )
            
            if len(points_3d) < 10:
                print(f"      âŒ é»é›²ä¸è¶³ï¼Œè·³é")
                del object_mask, points_3d
                gc.collect()
                torch.cuda.empty_cache()
                continue
            
            bbox_3d = self.compute_3d_bounding_box(points_3d)
            
            if bbox_3d is None:
                print(f"      âŒ ç„¡æ³•è¨ˆç®— 3D bboxï¼Œè·³é")
                del object_mask, points_3d
                gc.collect()
                torch.cuda.empty_cache()
                continue
            
            print(f"      âœ“ 3D bbox è¨ˆç®—æˆåŠŸ ({len(points_3d)} å€‹é»)")
            
            # ç¬¬äº”æ­¥ï¼šæå– 3D è³‡è¨Š
            info_3d = self._extract_3d_info(bbox_3d)
            
            print(f"      æå– 3D è³‡è¨Š...")
            print(f"      3Dä¸­å¿ƒ: ({info_3d['center_base'][0]:.3f}, {info_3d['center_base'][1]:.3f}, {info_3d['center_base'][2]:.3f}) mm")
            print(f"      3Då°ºå¯¸: ({info_3d['size_3d'][0]:.3f}, {info_3d['size_3d'][1]:.3f}, {info_3d['size_3d'][2]:.3f}) mm")
            
            # ç¬¬å…­æ­¥ï¼šè¨ˆç®—ç«¯é»
            endpoints = self._calculate_endpoints(bbox_3d, info_3d['center_3d'])
            
            print(f"      å·¦ç«¯é»åŸºåº§: ({endpoints['left_base'][0]:.3f}, {endpoints['left_base'][1]:.3f}, {endpoints['left_base'][2]:.3f}) mm")
            print(f"      å³ç«¯é»åŸºåº§: ({endpoints['right_base'][0]:.3f}, {endpoints['right_base'][1]:.3f}, {endpoints['right_base'][2]:.3f}) mm")
            print(f"      é•·é‚Šé•·åº¦: {endpoints['distance']:.3f} m")
            
            # ç¬¬ä¸ƒæ­¥ï¼šCLIP é©—è­‰
            print(f"      CLIP é©—è­‰...")
            with torch.no_grad():
                final_label = self._verify_with_clip(image_source, (x1, y1, x2, y2), object_mask)
            
            any_detected = True
            
            # æ”¶é›†æ‰€æœ‰è³‡è¨Šï¼ˆç°¡åŒ–ç‰ˆç„¡ region_type å’Œæ¡æŸ„è³‡è¨Šï¼‰
            detection_info = {
                'bbox_3d': bbox_3d,
                'bbox_2d': (x1, y1, x2, y2),
                '3d_info': info_3d,
                'endpoints': endpoints,
                'region_type': 'object',  # ç°¡åŒ–ç‰ˆå›ºå®šç‚º object
                'final_label': final_label,
                'logit': logit,
                'points_3d': points_3d
            }
            
            # ç¬¬å…«æ­¥ï¼šè¦–è¦ºåŒ–ï¼ˆç°¡åŒ–ç‰ˆè¼¸å‡ºï¼‰
            vis = self._visualize_detection_simple(vis, detection_info, color, W, H, det_idx)
            
            # ç¬¬ä¹æ­¥ï¼šå„²å­˜è³‡è¨Šï¼ˆç°¡åŒ–ç‰ˆï¼‰
            self._save_object_info_simple(detection_info)
        
        # å„²å­˜çµæœ
        vis_bgr = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)
        result_path = os.path.join(self.output_dir, "detection_3d_simple.jpg")
        cv2.imwrite(result_path, vis_bgr)
        
        if not any_detected:
            print(f"   âŒ ç›¸æ©Ÿ {self.camera_id}: æœªåµæ¸¬åˆ°ä»»ä½•ç‰©å“")
            self.clear_detection_data()
            return False
        else:
            print(f"\n   âœ… ç›¸æ©Ÿ {self.camera_id}: 3Dåµæ¸¬çµæœï¼ˆç°¡åŒ–ç‰ˆï¼‰å·²å„²å­˜: {result_path}")
            print(f"   æˆåŠŸæª¢æ¸¬åˆ° {len(self.objects_info)} å€‹ç‰©å“")
            self.clear_detection_data()
            return True
    
    def _visualize_detection_simple(self, vis, info, color, W, H, det_idx):
        """ç°¡åŒ–ç‰ˆç¹ªè£½æª¢æ¸¬çµæœï¼ˆç„¡æ¡æŸ„è³‡è¨Šï¼‰"""
        bbox_3d = info['bbox_3d']
        center_base = info['3d_info']['center_base']
        center_3d = info['3d_info']['center_3d']
        size_3d = info['3d_info']['size_3d']
        yaw = info['3d_info']['yaw']
        pitch = info['3d_info']['pitch']
        endpoints = info['endpoints']
        final_label = info['final_label']
        logit = info['logit']
        
        # ç¹ªè£½ 3D bbox
        vis = self.visualize_3d_bbox(vis, bbox_3d, color)
        
        # ç¹ªè£½ä¸­å¿ƒé»
        center_2d = self.project_3d_to_2d(center_3d)
        if center_2d is not None:
            cx, cy = center_2d
            cv2.circle(vis, (int(cx), int(cy)), 10, color, -1)
            cv2.circle(vis, (int(cx), int(cy)), 10, (255, 255, 255), 2)
            cv2.putText(vis, "C", (int(cx) - 20, int(cy) - 20), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
        # ç¹ªè£½ç«¯é»
        left_endpoint_2d = endpoints['left_2d']
        right_endpoint_2d = endpoints['right_2d']
        
        if left_endpoint_2d and right_endpoint_2d:
            Lx, Ly = left_endpoint_2d
            Rx, Ry = right_endpoint_2d
            
            cv2.circle(vis, (Lx, Ly), 8, (0, 255, 0), -1)
            cv2.circle(vis, (Rx, Ry), 8, (0, 0, 255), -1)
            cv2.line(vis, (Lx, Ly), (Rx, Ry), (255, 255, 255), 3)
            cv2.putText(vis, "L", (Lx - 20, Ly - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(vis, "R", (Rx + 10, Ry - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ç°¡åŒ–ç‰ˆæ–‡å­—è³‡è¨Šï¼ˆç„¡ Region å’Œ Edge Yawï¼‰
        text = (f"#{det_idx+1} {final_label}: {logit:.2f}\n"
            f"Center: ({center_base[0]:.3f}, {center_base[1]:.3f}, {center_base[2]:.3f})mm\n"
            f"Size: ({size_3d[0]:.3f}, {size_3d[1]:.3f}, {size_3d[2]:.3f})mm\n"
            f"Yaw: {yaw:.1f} deg\n"
            f"Pitch: {pitch:.1f} deg")
        
        font, fs, th = cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1
        lines = text.split('\n')
        line_height = cv2.getTextSize("Test", font, fs, th)[0][1] + 5
        
        tx = info['bbox_2d'][0]
        ty = info['bbox_2d'][1] - 10
        
        max_width = max([cv2.getTextSize(line, font, fs, th)[0][0] for line in lines])
        total_height = len(lines) * line_height
        
        tx = max(0, min(tx, W - max_width - 10))
        ty = max(total_height, min(ty, H - 10))
        
        cv2.rectangle(vis, (tx, ty - total_height), (tx + max_width, ty + 5), 
                    color, cv2.FILLED)
        
        for i, line in enumerate(lines):
            y_pos = ty - total_height + (i + 1) * line_height
            cv2.putText(vis, line, (tx, y_pos), font, fs, (0, 0, 0), th, cv2.LINE_AA)
        
        return vis
    
    def _save_object_info_simple(self, info):
        yaw = info['3d_info']['yaw']
        if yaw <0:
            yaw = yaw+180
        
        if info['3d_info']['size_3d'][2] > info['3d_info']['size_3d'][0] and info['3d_info']['size_3d'][2] > info['3d_info']['size_3d'][1]:
            pick_mode="side"
        else:
            pick_mode="down"
        
        """ç°¡åŒ–ç‰ˆå„²å­˜ç‰©å“è³‡è¨Šï¼ˆç„¡æ¡æŸ„è³‡è¨Šï¼‰"""
        obj_info = {
            "name": info['final_label'],
            "region_type": "object",
            "center_pos": tuple(info['3d_info']['center_base']),
            "3d_size": tuple(info['3d_info']['size_3d']),
            "angle":float(yaw),
            "left_endpoint": tuple(info['endpoints']['left_base']),
            "right_endpoint": tuple(info['endpoints']['right_base']),
            "camera_id": self.camera_id,
            "confidence": float(info['logit']),
            "pick_mode":pick_mode,
        }
        
        self.objects_info.append(obj_info)


    
    def get_objects_info(self):
        """å–å¾—åµæ¸¬çµæœ"""
        print(f"\nğŸ“‹ ç›¸æ©Ÿ {self.camera_id} ç‰©å“åµæ¸¬çµæœ:")
        for obj in self.objects_info:
            print(f" - {obj['name']}: {obj['confidence']:.2f}")
            print(f"   3Dä¸­å¿ƒ: {obj['center_pos']}, å°ºå¯¸: {obj['3d_size']}, æ–¹å‘: {obj['angle']}")
           
        return self.objects_info
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.pipeline:
            self.pipeline.stop()
        print(f"âœ“ ç›¸æ©Ÿ {self.camera_id} å·²é‡‹æ”¾")
        shared_models.release()
    def clear_detection_data(self):
        """æ¸…ç†åµæ¸¬å¿«å–è³‡æ–™"""
 
        
        # æ¸…ç©ºç‰©å“è³‡è¨Šåˆ—è¡¨
        self.objects_info.clear()
        
        # æ¸…ç©ºåœ–åƒå¿«å–
        self.latest_color_image = None
        self.latest_depth_frame = None
        self.depth_image = None
        
        # ğŸ”‘ é—œéµï¼šæ¸…ç† SAM predictor çš„åœ–åƒå¿«å–
        if hasattr(self, 'predictor') and hasattr(self.predictor, 'model'):
            try:
                # æ¸…ç† image encoder çš„å¿«å–
                self.predictor.model.image_encoder.features = None
                self.predictor.image_embedding = None
                
                # é‡ç½® image_size
                self.predictor.image_size = None
                
            except Exception as e:
                pass
        
        # åƒåœ¾å›æ”¶
        gc.collect()
        torch.cuda.empty_cache()
        
        print(f"âœ“ ç›¸æ©Ÿ {self.camera_id} çš„åµæ¸¬è³‡æ–™å·²æ¸…ç†")

    def clear_detection_data(self):
        """æ¸…ç†æª¢æ¸¬éç¨‹ä¸­çš„æ‰€æœ‰æš«å­˜è³‡æ–™ï¼ˆå¾¹åº•ç‰ˆï¼‰"""
        
        
        print(f"   [æ¸…ç†] é–‹å§‹æ¸…ç†ç›¸æ©Ÿ {self.camera_id} çš„æª¢æ¸¬è³‡æ–™...")
        
        # è¨˜éŒ„æ¸…ç†å‰çš„è¨˜æ†¶é«”
        if torch.cuda.is_available():
            mem_before = torch.cuda.memory_allocated() / 1024**2
            reserved_before = torch.cuda.memory_reserved() / 1024**2
            print(f"   [æ¸…ç†] æ¸…ç†å‰ - å·²åˆ†é…: {mem_before:.1f}MB, å·²ä¿ç•™: {reserved_before:.1f}MB")
        
        # 1ï¸âƒ£ æ¸…ç†æª¢æ¸¬çµæœ
        if hasattr(self, 'detected_objects'):
            for obj in self.detected_objects:
                for key in list(obj.keys()):
                    if key in ['mask', 'depth_data', 'point_cloud', 'image', 'crop_image', 
                            'bbox_3d', 'points_3d', 'handle_features']:
                        try:
                            del obj[key]
                        except:
                            pass
                obj.clear()
            self.detected_objects.clear()
            del self.detected_objects
            self.detected_objects = []
        
        # 2ï¸âƒ£ â­ é—œéµï¼šå®Œå…¨é‡ç½® SAM predictorï¼ˆé‡æ–°å‰µå»ºï¼‰
        if hasattr(self, 'predictor') and self.predictor is not None:
            # å˜—è©¦å®˜æ–¹é‡ç½®
            try:
                self.predictor.reset_image()
            except:
                pass
            
            # â­ åˆªé™¤ predictor çš„æ‰€æœ‰å…§éƒ¨å¼µé‡
            if hasattr(self.predictor, 'features'):
                del self.predictor.features
            if hasattr(self.predictor, 'original_size'):
                del self.predictor.original_size
            if hasattr(self.predictor, 'input_size'):
                del self.predictor.input_size
            if hasattr(self.predictor, 'is_image_set'):
                self.predictor.is_image_set = False
            
            # â­ é—œéµï¼šåˆªé™¤ image_embeddingï¼ˆæœ€å¤§çš„è¨˜æ†¶é«”ä½”ç”¨ï¼‰
            if hasattr(self.predictor, 'image_embedding'):
                del self.predictor.image_embedding
                self.predictor.image_embedding = None
            
            # æ¸…é™¤ interm_featuresï¼ˆä¸­é–“ç‰¹å¾µï¼‰
            if hasattr(self.predictor, 'interm_features'):
                if self.predictor.interm_features is not None:
                    for feat in self.predictor.interm_features:
                        if feat is not None:
                            del feat
                self.predictor.interm_features = None
        
        # 3ï¸âƒ£ æ¸…ç†å¯èƒ½çš„ GroundingDINO å¿«å–
        # æŸäº›ç‰ˆæœ¬çš„ GroundingDINO æœƒå¿«å–ç‰¹å¾µ
        if hasattr(self, 'gd_model'):
            if hasattr(self.gd_model, 'cache'):
                try:
                    self.gd_model.cache.clear()
                except:
                    pass
        
        # 4ï¸âƒ£ æ¸…ç† CLIP çš„å¿«å–ç‰¹å¾µ
        if hasattr(self, 'handle_reference_features'):
            # ä¸åˆªé™¤æ¡æŸ„ç‰¹å¾µï¼ˆéœ€è¦é‡è¤‡ä½¿ç”¨ï¼‰ï¼Œä½†æ¸…é™¤è‡¨æ™‚å‰¯æœ¬
            if hasattr(self, '_temp_handle_features'):
                del self._temp_handle_features
        
        # 5ï¸âƒ£ å¼·åˆ¶ Python åƒåœ¾å›æ”¶ï¼ˆåŸ·è¡Œå¤šæ¬¡ï¼‰
        print(f"   [æ¸…ç†] åŸ·è¡Œåƒåœ¾å›æ”¶...")
        total_collected = 0
        for i in range(5):
            collected = gc.collect()
            total_collected += collected
            if collected > 0:
                print(f"   [æ¸…ç†]   ç¬¬ {i+1} æ¬¡ GC: å›æ”¶ {collected} å€‹ç‰©ä»¶")
        print(f"   [æ¸…ç†] ç¸½å…±å›æ”¶ {total_collected} å€‹ç‰©ä»¶")
        
        # 6ï¸âƒ£ â­ é—œéµï¼šå¼·åˆ¶æ¸…ç©º CUDA å¿«å–ï¼ˆå¤šæ¬¡ï¼‰
        if torch.cuda.is_available():
            print(f"   [æ¸…ç†] æ¸…ç©º CUDA å¿«å–...")
            
            # åŸ·è¡Œå¤šæ¬¡ empty_cacheï¼ˆç¢ºä¿å¾¹åº•æ¸…ç†ï¼‰
            for _ in range(5):
                torch.cuda.empty_cache()
            
            # åŒæ­¥æ‰€æœ‰ CUDA æ“ä½œ
            torch.cuda.synchronize()
            
            # â­ å˜—è©¦é‡ç½®è¨˜æ†¶é«”çµ±è¨ˆï¼ˆæœ‰æ™‚æœ‰å¹«åŠ©ï¼‰
            try:
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.reset_accumulated_memory_stats()
            except:
                pass
            
            # è¨˜éŒ„æ¸…ç†å¾Œçš„è¨˜æ†¶é«”
            mem_after = torch.cuda.memory_allocated() / 1024**2
            reserved_after = torch.cuda.memory_reserved() / 1024**2
            mem_freed = mem_before - mem_after
            reserved_freed = reserved_before - reserved_after
            
            print(f"   [æ¸…ç†] æ¸…ç†å¾Œ - å·²åˆ†é…: {mem_after:.1f}MB, å·²ä¿ç•™: {reserved_after:.1f}MB")
            print(f"   [æ¸…ç†] é‡‹æ”¾ - å·²åˆ†é…: {mem_freed:.1f}MB, å·²ä¿ç•™: {reserved_freed:.1f}MB")
            
            # å¦‚æœé‡‹æ”¾çš„è¨˜æ†¶é«”å¾ˆå°‘ï¼Œç™¼å‡ºè­¦å‘Š
            if mem_freed < 50:
                print(f"   âš ï¸  è­¦å‘Š: åªé‡‹æ”¾äº† {mem_freed:.1f}MBï¼Œå¯èƒ½æœ‰è¨˜æ†¶é«”æ´©æ¼")
                print(f"   âš ï¸  ä¿ç•™ä½†æœªåˆ†é…: {reserved_after:.1f}MB")
        
        print(f"   âœ“ ç›¸æ©Ÿ {self.camera_id} çš„æª¢æ¸¬è³‡æ–™å·²æ¸…ç†")

